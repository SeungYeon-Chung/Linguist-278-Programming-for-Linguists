{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Linguist 278: Programming for Linguists<br/>\n",
    "Stanford Linguistics, Fall 2020<br/>\n",
    "Christopher Potts</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Assignment-7:-Language-dataset-hackathon\">Assignment 7: Language dataset hackathon<a class=\"anchor-link\" href=\"#Assignment-7:-Language-dataset-hackathon\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Distributed 2020-11-02<br/>\n",
    "Due 2020-11-09 (but my intention is that you will be able to turn it in after class on Nov 4)</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Contents\">Contents<a class=\"anchor-link\" href=\"#Contents\">¶</a></h2><ol>\n",
    "<li><a href=\"#Overview\">Overview</a><ol>\n",
    "<li><a href=\"#Requirements\">Requirements</a></li>\n",
    "<li><a href=\"#Ideas\">Ideas</a></li>\n",
    "</ol>\n",
    "</li>\n",
    "<li><a href=\"#Set-up\">Set-up</a></li>\n",
    "<li><a href=\"#Age-of-acquisition-dataset\">Age of acquisition dataset</a></li>\n",
    "<li><a href=\"#Concreteness-dataset\">Concreteness dataset</a></li>\n",
    "<li><a href=\"#Sentiment-dataset\">Sentiment dataset</a></li>\n",
    "<li><a href=\"#Beautiful-words\">Beautiful words</a></li>\n",
    "<li><a href=\"#Novels-from-Project-Gutenberg\">Novels from Project Gutenberg</a></li>\n",
    "<li><a href=\"#Potentially-useful-code\">Potentially useful code</a><ol>\n",
    "<li><a href=\"#Project-Gutenberg-iterator\">Project Gutenberg iterator</a></li>\n",
    "<li><a href=\"#Sentence-tokenizing-using-NLTK\">Sentence tokenizing using NLTK</a></li>\n",
    "<li><a href=\"#Word-counts\">Word counts</a></li>\n",
    "<li><a href=\"#egrep\">egrep</a></li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Overview\">Overview<a class=\"anchor-link\" href=\"#Overview\">¶</a></h2><ul>\n",
    "<li><p>Our in-class hackathon on November 2 and 4 <em>is</em> Assignment 7. My hope is that you can complete the hackathon/homework by the end of class on November 7.</p>\n",
    "</li>\n",
    "<li><p>You are encouraged to work in groups (max size: 3 people).</p>\n",
    "</li>\n",
    "<li><p>The requirements are much more open-ended than for other assignments. Part of the task is to think up an original question using the materials in this notebook. I've given some general ideas below to get you started.</p>\n",
    "</li>\n",
    "<li><p>You should think about scoping the problem so that you can complete it in two class periods. This is something you should assess for yourselves. Of course, you <em>can</em> take until Nov 9 if you like, but I'd advise not doing that!</p>\n",
    "</li>\n",
    "</ul>\n",
    "<h3 id=\"Requirements\">Requirements<a class=\"anchor-link\" href=\"#Requirements\">¶</a></h3><ul>\n",
    "<li><p>Designate one person to submit the notebook and make absolutely sure all group members names are given at the top of the noteboook.</p>\n",
    "</li>\n",
    "<li><p>Submit a modified version of this notebook, with your new code included, extraneous code removed, and prose added so that I can follow along. That is, try to make this notebook look like a polished piece of <a href=\"https://en.wikipedia.org/wiki/Literate_programming\">literate programming</a>.</p>\n",
    "</li>\n",
    "<li><p>I am guessing most notebooks will have about the scope of three regular two-pointer assignment questions, with some prose explaining what they do and why. However, there is no strict requirement on code quantity or anything.</p>\n",
    "</li>\n",
    "<li><p>You needn't confine yourself to the data and other resources in this notebook. (There is a lot to work with here, though.)</p>\n",
    "</li>\n",
    "</ul>\n",
    "<h3 id=\"Ideas\">Ideas<a class=\"anchor-link\" href=\"#Ideas\">¶</a></h3><p>Examples of things you might do (not meant to be restrictive!):</p>\n",
    "<ul>\n",
    "<li><p>Write code that identifies interesting relationships in the concreteness, sentiment, and age of aquisition datasets. You can use Pandas to merge them on their <code>Index</code> values and then reason across them!</p>\n",
    "</li>\n",
    "<li><p>Write code that identifies differences between Project Gutenberg authors as revealed by the concreteness, sentiment, age of acquisition, and/or beautiful words datasets.</p>\n",
    "</li>\n",
    "<li><p>Write code to find the <em>most something</em> – the most sentiment-laden sentence or passage, the most challenging passage, the most abstract passage, etc. I've included code below for heuristic paragraph and sentence parsing.</p>\n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Set-up\">Set-up<a class=\"anchor-link\" href=\"#Set-up\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Download the hackathon data distribution:</p>\n",
    "<p><a href=\"http://web.stanford.edu/class/linguist278/data/hackathon.zip\">http://web.stanford.edu/class/linguist278/data/hackathon.zip</a></p>\n",
    "<p>and unzip it in the same directory as this notebook. (If you want to put it somewhere else, just be sure to change <code>data_home</code> in the next cell.)</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_home = \"hackathon\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Age-of-acquisition-dataset\">Age of acquisition dataset<a class=\"anchor-link\" href=\"#Age-of-acquisition-dataset\">¶</a></h2><p>From <a href=\"https://www.humanities.mcmaster.ca/~vickup/Kuperman-BRM-2012.pdf\">Age-of-acquisition ratings for 30 thousand English words</a> (Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc Brysbaert, <em>Behavior Research Methods</em>, 2014):</p>\n",
    "<ol>\n",
    "<li><code>Word</code>: The word (str)</li>\n",
    "<li><code>OccurTotal</code>: token count in their data</li>\n",
    "<li><code>OccurNum</code>: Participants who gave an age-of-acquisition, rather than saying \"Unknown\"</li>\n",
    "<li><code>Rating.Mean</code>: mean age of aquisition in years of age</li>\n",
    "<li><code>Rating.SD</code>: standard deviation of the distribution of ages of acquisition</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "age_df = pd.read_csv(\n",
    "    os.path.join(data_home, \"Kuperman-BRM-data-2012.csv\"),\n",
    "    index_col='Word')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "age_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "age_df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Concreteness-dataset\">Concreteness dataset<a class=\"anchor-link\" href=\"#Concreteness-dataset\">¶</a></h2><p>We've worked with this dataset before. It's presented in <a href=\"http://crr.ugent.be/papers/Brysbaert_Warriner_Kuperman_BRM_Concreteness_ratings.pdf\">Concreteness ratings for 40 thousand generally known English word lemmas</a> (Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman, <em>Behavior Research Methods</em>, 2014). Overview:</p>\n",
    "<ol>\n",
    "<li><code>Word</code>: The word (str)</li>\n",
    "<li><code>Bigram</code>: Whether it is a single word or a two-word expression</li>\n",
    "<li><code>Conc.M</code>: The mean concreteness rating</li>\n",
    "<li><code>Conc.SD</code>: The standard deviation of the concreteness ratings (float)</li>\n",
    "<li><code>Unknown</code>: The number of persons indicating they did not know the word</li>\n",
    "<li><code>Total</code>: The total number of persons who rated the word</li>\n",
    "<li><code>Percent_known</code>: Percentage of participants who knew the word</li>\n",
    "<li><code>SUBTLEX</code>: The SUBTLEX-US frequency count</li>\n",
    "<li><code>Dom_Pos</code>: The part-of-speech where known</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concreteness_df = pd.read_csv(\n",
    "    os.path.join(data_home, \"Concreteness_ratings_Brysbaert_et_al_BRM.csv\"),\n",
    "    index_col='Word')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concreteness_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concreteness_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Sentiment-dataset\">Sentiment dataset<a class=\"anchor-link\" href=\"#Sentiment-dataset\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>The dataset <a href=\"https://www.humanities.mcmaster.ca/~vickup/Warriner-etal-BRM-2013.pdf\">Norms of valence, arousal, and dominance for 13,915 English lemmas</a> (Amy Beth Warriner, Victor Kuperman, and Marc Brysbaert, <em>Behavior Research Methods</em>, 2013) contains a lot of sentiment information about +13K words. The following code reads in the full dataset and then restricts to just the mean ratings for the three core semantic dimensions:</p>\n",
    "<ol>\n",
    "<li><code>Word</code>: The word (str)</li>\n",
    "<li><code>Valence</code> (positive/negative)</li>\n",
    "<li><code>Arousal</code> (intensity)</li>\n",
    "<li><code>Dominance</code></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_df = pd.read_csv(\n",
    "    os.path.join(data_home, \"Warriner_et_al emot ratings.csv\"),\n",
    "    index_col='Word')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_df = sentiment_df[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "\n",
    "sentiment_df = sentiment_df.rename(\n",
    "    columns={'V.Mean.Sum': 'Valence',\n",
    "             'A.Mean.Sum': 'Arousal',\n",
    "             'D.Mean.Sum': 'Dominance'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Beautiful-words\">Beautiful words<a class=\"anchor-link\" href=\"#Beautiful-words\">¶</a></h2><p>I took the <a href=\"http://www.alphadictionary.com/articles/100_most_beautiful_words.html\">100 Most Beautiful Words</a> (of which there are 107) and enriched them:</p>\n",
    "<ol>\n",
    "<li><code>Word</code>: The word (str).</li>\n",
    "<li><code>Pronunciation</code>: CMU Pronouncing Dictionary representation.</li>\n",
    "<li><code>Morphology</code>: Celex morphological representations.</li>\n",
    "<li><code>Frequency</code>: frequency according to the Google N-gram Corpus. </li>\n",
    "<li><code>Category</code>: 'most-beautiful' or 'regular'</li>\n",
    "</ol>\n",
    "<p>The 'regular' examples are 107 randomly selected non-proper-names.</p>\n",
    "<p>Maybe there's something interesting here?</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beauty_df = pd.read_csv(\n",
    "    os.path.join(data_home, \"wordbeauty.csv\"),\n",
    "    index_col=\"Word\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beauty_df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beauty_df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beauty_df['Category'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beauty_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Novels-from-Project-Gutenberg\">Novels from Project Gutenberg<a class=\"anchor-link\" href=\"#Novels-from-Project-Gutenberg\">¶</a></h2><p>The Gutenberg metadata has been removed from these files, and the first line gives the title, author, and publication year in a systematic pattern.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gutenberg_home = os.path.join(data_home, \"gutenberg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gutenberg_filenames = glob.glob(os.path.join(gutenberg_home, \"*.txt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(gutenberg_filenames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gutenberg_filenames[: 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Potentially-useful-code\">Potentially useful code<a class=\"anchor-link\" href=\"#Potentially-useful-code\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Project-Gutenberg-iterator\">Project Gutenberg iterator<a class=\"anchor-link\" href=\"#Project-Gutenberg-iterator\">¶</a></h3><p>You might want to modify this, depending on how you want to process these texts (by word? sentence? chapter?).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gutenberg_iterator(filename):\n",
    "    \"\"\"Yields paragraphs (as defined simply by multiple\n",
    "    newlines in a row).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Full path to the file.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    multiline str\n",
    "\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        contents = f.read()\n",
    "    for para in re.split(r\"[\\n\\s*]{2,}\", contents):\n",
    "        yield para\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emma_iterator = gutenberg_iterator(gutenberg_filenames[0])\n",
    "\n",
    "for _ in range(5):\n",
    "    print(\"=\"*50)\n",
    "    print(next(emma_iterator))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Sentence-tokenizing-using-NLTK\">Sentence tokenizing using NLTK<a class=\"anchor-link\" href=\"#Sentence-tokenizing-using-NLTK\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_tokenize(\"Hello? This is Dr. Potts! How are you?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"Word-counts\">Word counts<a class=\"anchor-link\" href=\"#Word-counts\">¶</a></h3><p>From assignment 2.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_tokenize(s):\n",
    "    \"\"\"Break str `s` into a list of str.\n",
    "\n",
    "    1. `s` has all of its peripheral whitespace removed.\n",
    "    2. `s` is downcased with `lower`.\n",
    "    3. `s` is split on whitespace.\n",
    "    4. For each token, any peripheral punctuation on it is stripped\n",
    "       off. Punctuation is here defined by `string.punctuation`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        The string to tokenize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "    \"\"\"\n",
    "    punct = string.punctuation\n",
    "    final_toks = []\n",
    "    toks = s.lower().strip().split()\n",
    "    for w in toks:\n",
    "        final_toks.append(w.strip(punct))\n",
    "    return final_toks\n",
    "\n",
    "\n",
    "def word_counts(s, tokenizing_func=simple_tokenize):\n",
    "    \"\"\"Count distribution for the words in `s` according to `tokenizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        String to tokenize and get  word counts for.\n",
    "    tokenizing_func : function\n",
    "        Any function that can be called as `tokenizing_func(s)` where\n",
    "        `s` is a string. The default is `simple_tokenize`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict mapping str to int\n",
    "    \"\"\"\n",
    "    wc = {}\n",
    "    toks = tokenizing_func(s)\n",
    "    for w in toks:\n",
    "        wc[w] = wc.get(w, 0) + 1\n",
    "    return wc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"egrep\">egrep<a class=\"anchor-link\" href=\"#egrep\">¶</a></h3><p>From assignment 5.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def egrep(regex, filename):\n",
    "    \"\"\"Python version of egrep. The function iterates through the\n",
    "    user's file `filename`, line-by-line, stripping off the final\n",
    "    newline character, and yielding only the lines that match the\n",
    "    user's regular expression `regex`.\n",
    "\n",
    "    Note: like basic egrep, a line that contains multiple matches for\n",
    "    `regex` is yielded only once.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regex : Compiled regular expression\n",
    "        The pattern to use for matching\n",
    "    filename : str\n",
    "        Full path to the file to open and iterate through\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    str\n",
    "        Lines from the file, with newline characters removed\n",
    "    \"\"\"\n",
    "    for line in open(filename):\n",
    "        line = line.strip()\n",
    "        if regex.search(line):\n",
    "            yield line\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
